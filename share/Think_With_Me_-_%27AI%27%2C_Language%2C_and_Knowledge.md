---
title: Think With Me - 'AI', Language, and Knowledge
dateCreated: 2023-11-11 00:55
cssclasses: 
aliases: 
- ""
- ""
---
Related: 
# Think With Me - 'AI', Language, and Knowledge

## Summary
==BEGIN AI-GENERATED CONTENT==
In this lengthy monologue, Corey discusses his perspective on language models, viewing them as impressive compressions of everything ever written. He explains how language models work, breaking down words into unique parts or tokens, and using probability to predict what comes next. He also discusses the limitations of language models, such as their inability to create new information or correct themselves if they go off track. Corey emphasizes the importance of understanding the source of information, and the confidence that comes from being able to cite sources accurately. He also shares his approach to learning, which involves reading books slowly and taking notes to ensure a deep understanding of the material. He concludes by encouraging listeners not to make life harder than it needs to be, and to strive to make others' days better.
==END AI-GENERATED CONTENT==

## Transcript
Corey (00:00):
I had like to posit my idea of thinking of a language model as an impressive compression of everything that's ever been written. In essence, I was thinking about how words are just words in the end. There's a finite number of words that each person not that is in the world. There's a seemingly infinite amount of words in the world, many, many, many new ones being made up every day, but there's only a finite number of words that any person will know in their life. And so that kind of creates an upper bound of what they're able to learn. It doesn't create an upper bound on their ability to learn. They can learn new words, but they might first have to learn new words before they can learn more important new words. That's why we have roots. And suffixes and affixes. Anyway, but it's all words. The words that get somebody into religion are the same words that could get somebody out of a religion. The words that one can say to somebody can compel them to jump, and those same words in a different order can compel them to get off. But it's all words, right? That's it. But

(01:55):
We've now taken all words

(02:02):
Because the thing about tokenizing a language model is so that you don't have to store words. You don't have to store every single word imaginable in a dictionary. You only have to store each unique part, which in some cases is whole words, but in a lot of cases it's a weird combination of suffix and suffix and suffix and suffix. It's little sections of words, right? Like second has the same part as the word sect, so you can describe section and sect with the same. And actually those both, and that's a bonus because sect is a word, but also section is a word, but section is just sect and then with the suffix chin on it, ION, which is a standard suffix so that you can use that anywhere else. And so by not being based actually on human words, but just of what combination of characters make up human words, there's an incredible amount of diversity that a language model can just store as data. It's just as what comes next? If I'm talking about writing a book or if I'm talking about US code, when the generator comes up to sect, it either is going to produce a space or it's going to produce in say, so either I'm saying sect or section. Well, the rest of the context previously has been about law, so the math is going to say section. It's not going to say we're talking about a sect, whereas if they were talking about say, cult history, then it might be more likely to say sect rather than section.

(03:48):
But it's all in theory if you can, because it's been, I have no idea how to explain this in any sort of way, but if I tuned it or I trained it, say with a book starting from the beginning and going to the end and I said, starting with these 10 words, this is all what's going to come next? And then I do that again with all of this other information and it all evens out and I balances and there's normalization happening. But then if I fed it those same 10 words that started the book and then asked it to continue out the probabilistic rest of that book, in theory, it could reproduce the book, but it doesn't take up space like disc space for context. The smallest of these models that I've seen are like 15, I think. Well, not smallest. What I'm recalling is 15 gigabytes, which is maybe a third of a video game nowadays. I think if you were to download a big AAA game, it'd probably be 50 to 75 gigabytes. It would also be three or four Blu-ray movies or three or four, three gig, decent quality torn in movies. Not that I know what that is. From experience, I read a lot on the internet. That's why I know those numbers,

(05:28):
Which is tiny to in theory hold all of human information or at least all possibilities of human speech, of language. That's kind of what it implies to be able to create, because in theory, there are zero. Well, yeah, actually what tokens are there absolutely no combinations of because those would be sequences of letters, not words per se, but those would be sequences of letters that have never existed in any asterisk of the training data that these models were trained on. It's not all of human knowledge. It's not everything that's ever been written. It is just whatever they've been trained on. That's a fact. It cannot create new information and recognize it as new information as far as it is concerned, everything it is saying is made up because it's all probability. It just happens to then translate back to us, back to English, but note that all the math is done on the binary code of the letters as then as tokenized and everything, and so it's very impressive to have all of human knowledge in theory. Currently using is limited to only the words that have actually been used in the training data. No, actually, because it can make up new words, so it can, well, yeah, I think that's an interesting, because then if you set, could you then train it to then choose a token sequence that has never been seen before?

(07:34):
So say sect to, I don't know, a whole different word like sec. Yeah, there we go. Just for example, sect say that's say that that has never existed until now. If I used that, well, if I used it in the prompt, then it would use it, then it would exist because the inference engine, which is the actual thing that actually takes the language model, which is a big binary file, it is basically a movie, just all neural network connection, association information, data, which I guess that's why I think about everything in terms of words and sentences. Now, I'm still a very visual learner and everything, but especially since I use monospace by default, which just, I mean it turns words into lines of letters for me and not so much a word where you can see that the word says cal and it's longer, so it's like, okay, that's California. And then because I didn't actually spend the time to read in my head, California, I think of all rather, I try to think all of my thoughts, oh, that's it. I try to think all of my thoughts in a way that is as expressible in plain English

(09:12):
Because I don't, at this point, I'm like, I don't consider a thought to have been useful or permalink if I didn't say it out loud, at least say it out loud or then going into the effort of recording it or then writing it down or typing it up or any of that. But if I just think it, it's like, okay, that's useful, but if I ever need to express this again, I won't be able to as easily as if I just thought about this in English in essence. So I believe that's probably why I am so long-winded now because I kind of found the mental model of thinking, not thinking like an AI per se, but rather thinking of language as

(10:12):
In kind of a visual mathematical way of being. All of languages exists in a set that I can pull out of that will the words that I want to use, and as I'm thinking and as I'm zooming into an idea, then as I get closer to the idea that I'm trying to express, all the synonyms start to pop out, so it's kind of like a nested tree in a sense. As you go down the tree of thoughts, you get more and more synonyms and I can just pick and choose the synonym that I want to use, but I kind of do it all sequentially now, I guess rather than trying to multitask in my head because people don't really do that, but it causes trouble because then I get sidetracked really easily, like now and then I often forget what I was saying originally and I realized that that is language. Language models, yes, S compression, that's it. Compression of language. Well, the compression of human language as it's been expressed authentically until we get to reality. Now that we are creating, we are letting language models create training data for other language models, which is not, there's nothing wrong with that, and there's no allusion to AI takeover in that way. It's rather just the simple idea of garbage in, garbage out. If the things that the AI that was trained on human data don't reflect human data and then that unreflective human data is used for another language model, then cool, now we've got bad human information

(12:36):
Squared. Great. There's no validation at least by default. Obviously, if it's using search engines or if it's using wolf ram, then there is a computational, something that's not based in probability that's happening because if you ask it, how many days are there in a year it'll say, or if you ask it to say, tell me in three digits how many days there are in a year, it'll say both 365 and 366. Maybe it'll say one. Maybe it'll say the other. It all depends on the probability it, but it's not, there's no reasoning behind it other than just what is the next thing to come after this sequence of tokens where the sequence is how many days are there in a year answering three tokens? Is it 3, 5, 5 or 3, 5, 6? Well, the probability of all of the, it is 0.97 for 365 and 0.93 for 366. However, the temperature has been changed on this model, and so we're actually going to probabilistically decide to go with the less likely option to shake things up a bit, and so it responds with 366. That's it. That's the reasoning that because it also doesn't go back. There's a linearity to how it generates text where because it's not generating text, it's generating tokens. If you could train the same model with English data and Chinese data, and it would work the same for each of those because at its core the languages are being broken down into binary and then they're being acted on, and that's where all the probabilistic, that's what all the probability is pertaining to is these digits.

(15:01):
Well, these matches in the token because the token is I think, deterministic in how it sequence or how, I don't know if the token has a dictionary. I think it does. I think the token token has a dictionary, and if I recall, it's something in the vein of 32, 30 5,000 individual tokens, a lot of which are words, but a lot of which are also parts of words and punctuation and other using any possible digit or any possible glyph that can be expressed in Unicode or ask E, so that's how it let say works. Let's see. That's how it works on the backend unit code. Oh yeah, English and Chinese. It's all being acted on in the same way in that it's all just binary, binary information and what binary information is likely to happen next given this previous line of binary information,

(16:15):
But it doesn't go back. It's not going to realize it's gone off track and then edit everything it just said in real time. A person might be able to do, once it commits to a token, it commits to a token, which is really interesting to watch it live. If you study it enough, you can see, kind of see the tokenization in the patterns and you can see exactly where something happens, say where it starts using a list and you're like, well, yep, now it's going to keep going down a list because probabilistically, it has started. Now that there has been a list started, that sequence of tokens, it's likely to happen again because that's how it would be in human language, and that's what it's committed to, and I keep saying it as if it anthropomorphize what it's doing, what the engine is doing, because it's easier. It's just so much easier to anthropomorphize it knowing how it works on the backend and knowing the finitude of it.

(17:34):
I

(17:37):
Helps me understand better its limitations, but also its assets. I realize that, I mean, there's a finite set of what it can produce, but anything within that set, 99% of that is good enough for me for what I want to use it for, which is basically anything that I can call creative writing. I don't use it for facts, but I do use it for explain X in Y format, in which case then I might understand something better and I can go and verify the information elsewhere, or I can use it just as a writing assistant, just like, Hey, take this and rewrite it to be in this form or rewrite it to have this style and then I can check it manually because what it does at its core is it makes stuff up.

(18:33):
That is what it does, and if you can realize just how much there is in your life that you could just make up or rather just, I mean in terms of explanations, I use it a lot in explain non-computer science term concepts in computer science concepts and see how well it transposes ideas in that moment. I don't really care if it's factually correct. The mental exercise is what's useful to me of thinking about these two separate domains and letting an AI or letting a language model that is trained with all data, hopefully equally, how does it synthesize these two disparate domains with total objectivity, total equal exposure across like in her when she's just like, oh, yeah, no, I read all of human knowledge in 10 seconds because in essence, that's what the model, that's in essence, that's what it does to the end user is it is. It's all of all books available to humans trained into an amorphous blob, but yeah, there's a lot of advantage to be taken of it if you just outright accept and seek to manipulate the inherent inductive logic limitations of it. It makes it a lot,

(20:37):
It makes crafting prompts more intuitive. I think if you think of it as like, I'm narrowing down, I'm taking the entire of Wikipedia in essence plus a billion, a trillion other things, and I'm asking it to tell me a specific thing out of that entire section, it's going to be on a, if I don't give it enough narrower, right? My answer is going to be over broad. It's going to be way too surface level, but also if I try to tell it, if I try to have it explained to me something that's too esoteric or way too narrow, it's going to be over narrow, and that's where it starts making stuff up. That's where it starts hallucinating or one of the places is when you ask it to, if you ask it to make up to write the definition of a word that doesn't exist either, the model will tell you, oh, that word doesn't exist or it'll make something up, or that was one of my first uses was I made up a term that I knew was never used anywhere, but it was, I don't remember what it is, but it was clear what it was supposed to do just by the words.

(22:04):
It was just a combination of two words and it gave me a definition and I was like, that is really close to what I thought. Let, I know I have this in my notes, I just do not remember where it is in my dictionary note

(22:39):
I,

(23:06):
I'm not sure where it is. Okay. Anyway, but it made up a definition that was exactly what you would've gotten from a general person if you had asked them to try and put those two words together and ascertain what combined they might mean. If you just mash the dictionary, the dictionary definitions together, when two dictionary definitions love each other very much, they make a new word that means something more complex and then somehow less because in the end, I mean all compression is lossy, or at least this compression is lossy because you get it to verify you cannot.

(24:03):
I

(24:06):
Trust that it will give you a fact 100% of the time every time, even if it does give you a fact 100% of the time, every time as counted, its probability, and so it might give you a different answer and it won't be able to tell you why. You might ask it again right after, and it might give you the right answer again, and that's a limitation that's fixed a lot by pre and post-processing, and that's how the search engine ones work where they're like, Hey, here are, I mean presumably, actually, that's a good question. I'm not sure. Is it two separate systems that gives you the AI response and then gives you the sources? It's cited or did it search or is it searching and then dumping in all of the sources, and then basically, I'm looking at a lot of this, I guess through the idea, the broad idea of chain of custody city where I presume there is a source for all facts somewhere for math. For math, it's a computational engine, right? It's a calculator or it's a calculator on steroids like the calculator by definition in its logic in how the physics of it work, how the transistor exists within our laws of physics. The calculator should not be wrong mathematically, but a person can be and a person can come to the wrong conclusion with the right logic,

(26:06):
But once we trust that something is written down somewhere or that it's recorded, we trust that that is our single source of truth. If you want to know, for instance, if you want to know when a law becomes a law, it becomes a law when it actually, I don't know the answer to that entirely. There's two things. There's the federal register and

(26:35):
There's a physical book anyway, but there are two places when federal law becomes official that it is published to, and those are the single sources of truth for federal law. It's not even Cornell really. I mean, Cornell is, I think the law library and Cornell is enough like that. It's citable, but because we know, we trust that when we go to the Cornell website and we read it, we read it word for word, and we see it word for word for word for word, and we can verify that our understanding match, like how we read it matches what's on there, but if you had an AI just read through the code and then you asked the questions about it, it might be right, it might be wrong, it doesn't know, and then neither will you, because you can't even go back and see within the model, even if you ask it to quote, unless there's a processor in there that says, oh, here, pull in this exact quote, because here's the address. S. There's a lot of ways to ground, I think, ais into knowledge, and that's one of those ways is just by having a code, like

(27:54):
Having it craft the command for some external service and then executing that on the external service, so having the AI interpret the person's question and then translate it into an HTT P lookup or whatever they're called, I don't know, a standard API lookup, which would give you the chain of custody word for word transcription, and then that would be injected into the response from the ai and then that, so then you could ascertain that there's a chain of custody in essence from what you're seeing now to the link where it says, you can go to Cornell and then from Cornell you can see, oh, it's published in the Federal Register. Then you can go to the federal Register and you can verify that according to the US government, that is the language of the law, and that is the only place that it matters what the language is. I've become very obsessed with sources and with the chain of custody for ideas and just knowing where your information comes from, which I've seen within myself a very strengthened remembering of

(29:20):
Where ideas come from because I'm restricting myself not to a particular topic, but to a wide variety of higher level books from within those respective domains, but I'm keeping in with those books as my single sources of truth. If I didn't verify it within Grey's Anatomy or within my intro to Neuro or my Neuroscience for Dummies book, then this cool thing that I read on the internet might be is a cool fact, but it's just a neat thing that I need to verify. But what that means is I can very easily recall within all of the stuff I've been thinking about during the sabbatical, I can very easily recall where ideas came from, and I realized that that's important just tonight, because tonight I started reading meditations by Marcus Aurelius in the first book in that is him listing off who he learned all of his things from.

(30:24):
He cites his sources, he says, from my grandfather, this from my father, this from Maximus, this from detritus, this from epi, this. He cites his sources for all of the things that make him him, and that's not something that I've felt like I've been able to do before because I just wasn't amalgamation of a bunch of stuff I read on the internet and maybe verified, and then when I cut all of that out and I said, no, I want chain of custody. I want to know that if I'm wrong, then the book that I read was also wrong, and that's my backup, and that's the benefit of becoming obsessed with my trade and deciding to get really stinking good at computer science and then some intersecting domains as well as I figure out what those are, say like neuroscience. Then I'll extend my knowledge, neuroscience and with the motivation of bridging the gap between finding the gaps or finding the interesting gaps between computer science and neuroscience, so in that there's a confidence that I get that I didn't have before about asserting myself and my knowledge and my strong opinions about somewhat esoteric things, the intersections of some domains, and then a lot of how I understand the world or I choose to describe it in language,

(32:17):
But a lot of my understanding of the world will be wrong, and a lot of it is right now, and I'm working on undoing a lot of that, and I want to know going forward that if I'm wrong, it's not because I'm wrong, it's because somebody else was wrong, but also I can say exactly who that was and not just, well, it must've been something I read 10 years ago on Reddit. It's like, no, I read this in this book, and you know what? I actually, I'm strengthened in this position actually because I then connected that I remember this now about this other thing that I read about in this book, and then you could build an essay of a response using all of these different sources because from just within in your ramblings, because as part of the information where it came from, I know what I know about Alexander Hamilton because I've read, because I've listened to the musical and because I've read Ron Chernow's book and also actually another one that is the Alexander Hamilton's Guide to Life, which is very fun, very fun book. I do recommend it by Jeff Wilson.

(33:35):
Those are the three sources, this three majority sources for everything that I know about Alexander Hamilton, most of anything that I remember about Thomas Jefferson, or rather the information that I know about Thomas Jefferson that I remember to be unique to me, the fact that after his wife died, he just kind of layed around Monticello, kind of just sad moping, and I remember reading that and thinking, huh, that's kind of a privilege to just not work and just like, oh, woe is me. I guess I'll just, oh, I don't know about living, and so he did two or three years of moping before getting back into anything, and I know where that's from. That's from The Art of Power by John Meacham, because that's the only thing I've read about Thomas Jefferson majority. I haven't read any other books about Thomas Jefferson specifically, so I might have other aspects. Anyway, and if you think about it, I feel like that's kind of a lost skill is not just to quote people say, or quotes and quoting quotes. Is

(35:03):
I

(35:06):
Dull? I think because most quotes that people remember are not really quotes. They're more references, which are not hard to remember and not as impressive as remembering whole quotes. But not only do we not remember whole quotes, even I struggled to remember four or five lines of Hamlet, even though a couple months ago I remembered 11 or 12, I think,

(35:34):
But that was something that I noticed over the summer that was happening after I stopped reading news and dah, dah, dah, dah. Well, I was also rereading Hamlet and rewatching, but I was just remembering Hamlet lines, not because I was trying to memorize them any other time before that I would've perhaps wanted to memorize Hamlet. I was just recalling them and going, oh, that this two, two solely flesh would melt, thaw and resolve itself into aew or that the everlasting had not trend, and I remember them now, but I could, and I would just say them because I was like, oh, this is kind of fun to just remember and let this quote bubble up into my head.

(36:21):
But that's not only is it impressive to quote anything, but to quote passages and to quote where they're from and who thereby and what that was in almost like Good Hill goodwill hunting. I feel like in some places, in some books or with some books, I'm getting to that point, I'm imagining myself. I've always watched that been like, okay, that is actually really impressive, but I don't know, if you spent six weeks reading a book because you enjoyed reading it rather than the two weeks that you had to read it in school, then yeah, perhaps you would just remember this book more and you would recognize when whole passages of it are being used in a conversation in a bar, because I'm reading, that's what I'm doing. I spending, I am in the middle of 12 books and I'm taking my time, I'm taking my sweet time with 'em, but I am taking notes on them when I am reading them.

(37:31):
I'm actively thinking about each of them when I'm reading them, and often they're the only or one of two or three sources in a domain, so every new thought that I have in a particular domain is most likely stemming from something that I read in that book, and that's how I'm beginning to read these books and I'm like, oh, these are, yeah, you do just kind of remember whole passages, or at least the ideas of whole sections because I'm reading. I'm not trying to just get it done to read it from beginning to end for class or to get it done beginning to end, so I can say, I read this many books in a year. I'm reading it as I feel like I understand it, and if I don't understand it, I will move on. Maybe I'll go and find another explanatory source that explains that concept better, and then once I understand it, I'll verify that that understanding is correct with the original material and then I'll continue on.

(38:30):
It's a very linear process that is built on building a foundation on strong sources because then either I will be incredibly informed about the things that I talk about or I'll be incredibly informed and need to be corrected, but also my material needs to be corrected. So since my source is a whole book and not just like a random Reddit post that I forgot, there's much more of an internal, not less internal shame, I guess, at being wrong if you feel that if you don't react well to being wrong, and so you don't assert yourself enough, but if you know your source material, if you can cite your sources, at least for me, it gave me an incredible confidence in the things that I chose to speak about because I chose to speak about only the things that I feel confident in and I accept, and along with quitting the news, I accept that I really know nothing about the things that I thought I knew a little about just from exposed, being exposed to the news, but since I don't remember any of that right now, I truly do not understand any of that.

(40:00):
Somebody could tell me the entire paper, the entire front page today, and it would have no, I would have nothing in my life to attach it to, and so it'd kind of be in one ear, not the other, which for me is good. That's the intent because I wouldn't be able to cite that source because that source is probably going to be wrong the next day, and I wouldn't cite that source because I wouldn't be talking about that thing if all I knew about it was one person reading out the article to me.

(40:36):
But what I can do is say on Israel Palestine is say, spend six weeks diving, just choosing to independently dive into the entire thing, only looking at up to some cutoff point, choose a cut because that's how we do history. There's a cutoff point where things too recent aren't history and things after that point are history, so you read up on the history and all that, and you get your facts straight from beginning to end or from beginning to present. You get your facts straight on the first go. Then you get up to date on the news from the time you stopped now. Now, not only do you have an understanding of the conflict from your soft cutoff point to the present just based on the news, the trajectory, based on the news, the history well enough that you can confidently correct somebody else because that's the value that I see in having it and having knowledge is making sure that it's conserved and you do that more confidently if you trust that you're correct, and at least for me, I trust that I'm correct. If not only do I feel that I know my source, but that I can actually, as we started at the beginning of this that I, or did we, no, I don't know if that might've been a different one different episode.

(42:17):
It's not enough for me to feel like I know my source or feel like I have already verified that information is correct. I want to be able to express it in English. I want to be able to express it

(42:30):
By virtue of knowing it as a fact and not just knowing it as a well. Everything that I remember from this period of research time, I verified to be correct, and so I don't know what my source is, but I'm confident I don't want to do that because then if you are proven wrong, then it just looks bad for you. But if you can make your point, cite your source and then be proven wrong, okay, well, yeah, you were wrong, but you know what, so is that guy, the author that your source, that historian, it gives you an out and it's a book. I think that's it for today. Thank you. Thank you for listening. Have a good one. Stay safe and just don't make things hard. That's it. My request is just don't make anything harder than it has to be in your life, in anybody else's life. Just we are all here for about the same time amount of time. You got long enough to go. Don't need to make it any worse than it already is. If anything, you can make it better than they thought it could be, right? That because you don't know what anybody is feeling in any moment how they feel like their day is going, so if you can do something that just feels mildly enlightening to you, you've already had a great day. Might just make their day. It might make their entire day.

(44:23):
You never can tell.

